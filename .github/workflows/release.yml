name: Build & Release

on:
  push:
    tags:
      - 'v*'
  workflow_dispatch:
    inputs:
      version:
        description: 'Version to build (e.g., v1.4.1)'
        required: false

permissions:
  contents: write

env:
  LLAMA_CPP_VERSION: "b7770"
  PYTHON_VERSION: "3.11.9"
  NODE_VERSION: "20"

jobs:
  # ============================================
  # Windows CUDA Build
  # ============================================
  build-win-cuda:
    runs-on: windows-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: GUI/package-lock.json

      - name: Download llama.cpp CUDA binaries
        shell: pwsh
        run: |
          $url = "https://github.com/ggml-org/llama.cpp/releases/download/${{ env.LLAMA_CPP_VERSION }}/llama-${{ env.LLAMA_CPP_VERSION }}-bin-win-cuda-12.4-x64.zip"
          Write-Host "Downloading: $url"
          Invoke-WebRequest -Uri $url -OutFile llama-cuda.zip
          New-Item -ItemType Directory -Force -Path middleware/bin/win-cuda
          Expand-Archive -Path llama-cuda.zip -DestinationPath middleware/bin/win-cuda -Force
          # å¤„ç†å¯èƒ½çš„åµŒå¥—ç›®å½•
          $nested = Get-ChildItem middleware/bin/win-cuda -Directory | Where-Object { $_.Name -like "llama-*" }
          if ($nested) {
            Get-ChildItem $nested.FullName | Move-Item -Destination middleware/bin/win-cuda -Force
            Remove-Item $nested.FullName -Recurse
          }
          Get-ChildItem middleware/bin/win-cuda

      - name: Setup Embedded Python
        shell: pwsh
        run: |
          $url = "https://www.python.org/ftp/python/${{ env.PYTHON_VERSION }}/python-${{ env.PYTHON_VERSION }}-embed-amd64.zip"
          Write-Host "Downloading Python: $url"
          Invoke-WebRequest -Uri $url -OutFile python-embed.zip
          Expand-Archive -Path python-embed.zip -DestinationPath python_env -Force
          
          # Enable site-packages
          $pthFile = Get-ChildItem -Path python_env -Filter "python*._pth"
          (Get-Content $pthFile.FullName) -replace '#import site', 'import site' | Set-Content $pthFile.FullName
          
          # Install pip
          Invoke-WebRequest -Uri "https://bootstrap.pypa.io/get-pip.py" -OutFile get-pip.py
          python_env/python.exe get-pip.py --no-warn-script-location
          
          # Install dependencies (åŒ…å« server æ¨¡å—)
          python_env/python.exe -m pip install -r middleware/requirements.txt --target python_env/Lib/site-packages --no-warn-script-location
          python_env/python.exe -m pip install -r middleware/server/requirements.txt --target python_env/Lib/site-packages --no-warn-script-location

      - name: Install Node dependencies
        working-directory: GUI
        run: npm install

      - name: Build
        working-directory: GUI
        run: npm run build:win
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Rename artifact
        shell: pwsh
        run: |
          # å°† LICENSE å’Œ README å¤åˆ¶åˆ° dist ç›®å½•ï¼ˆelectron-builder ä¼šæ‰“åŒ…è¿› zipï¼‰
          Copy-Item "LICENSE" -Destination "GUI/dist/LICENSE.txt"
          Copy-Item ".github/release-docs/README-windows-cuda.md" -Destination "GUI/dist/README.md"
          # é‡å‘½å zip æ–‡ä»¶
          Get-ChildItem GUI/dist/*.zip | ForEach-Object {
            $newName = "Murasaki-Translator-${{ github.ref_name }}-win-cuda-x64.zip"
            Rename-Item $_.FullName -NewName $newName
          }

      - name: Upload Artifact
        uses: actions/upload-artifact@v4
        with:
          name: Murasaki-win-cuda-x64
          path: GUI/dist/*-cuda-x64.zip
          retention-days: 7

  # ============================================
  # Windows Vulkan Build
  # ============================================
  build-win-vulkan:
    runs-on: windows-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: GUI/package-lock.json

      - name: Download llama.cpp Vulkan binaries
        shell: pwsh
        run: |
          $url = "https://github.com/ggml-org/llama.cpp/releases/download/${{ env.LLAMA_CPP_VERSION }}/llama-${{ env.LLAMA_CPP_VERSION }}-bin-win-vulkan-x64.zip"
          Write-Host "Downloading: $url"
          Invoke-WebRequest -Uri $url -OutFile llama-vulkan.zip
          New-Item -ItemType Directory -Force -Path middleware/bin/win-vulkan
          Expand-Archive -Path llama-vulkan.zip -DestinationPath middleware/bin/win-vulkan -Force
          # å¤„ç†å¯èƒ½çš„åµŒå¥—ç›®å½•
          $nested = Get-ChildItem middleware/bin/win-vulkan -Directory | Where-Object { $_.Name -like "llama-*" }
          if ($nested) {
            Get-ChildItem $nested.FullName | Move-Item -Destination middleware/bin/win-vulkan -Force
            Remove-Item $nested.FullName -Recurse
          }

      - name: Setup Embedded Python
        shell: pwsh
        run: |
          $url = "https://www.python.org/ftp/python/${{ env.PYTHON_VERSION }}/python-${{ env.PYTHON_VERSION }}-embed-amd64.zip"
          Invoke-WebRequest -Uri $url -OutFile python-embed.zip
          Expand-Archive -Path python-embed.zip -DestinationPath python_env -Force
          $pthFile = Get-ChildItem -Path python_env -Filter "python*._pth"
          (Get-Content $pthFile.FullName) -replace '#import site', 'import site' | Set-Content $pthFile.FullName
          Invoke-WebRequest -Uri "https://bootstrap.pypa.io/get-pip.py" -OutFile get-pip.py
          python_env/python.exe get-pip.py --no-warn-script-location
          python_env/python.exe -m pip install -r middleware/requirements.txt --target python_env/Lib/site-packages --no-warn-script-location
          python_env/python.exe -m pip install -r middleware/server/requirements.txt --target python_env/Lib/site-packages --no-warn-script-location

      - name: Install & Build
        working-directory: GUI
        run: |
          npm install
          npm run build:win
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Rename artifact
        shell: pwsh
        run: |
          # å°† LICENSE å’Œ README å¤åˆ¶åˆ° dist ç›®å½•
          Copy-Item "LICENSE" -Destination "GUI/dist/LICENSE.txt"
          Copy-Item ".github/release-docs/README-windows-vulkan.md" -Destination "GUI/dist/README.md"
          # é‡å‘½å zip æ–‡ä»¶
          Get-ChildItem GUI/dist/*.zip | ForEach-Object {
            $newName = "Murasaki-Translator-${{ github.ref_name }}-win-vulkan-x64.zip"
            Rename-Item $_.FullName -NewName $newName
          }

      - name: Upload Artifact
        uses: actions/upload-artifact@v4
        with:
          name: Murasaki-win-vulkan-x64
          path: GUI/dist/*-vulkan-x64.zip
          retention-days: 7

  # ============================================
  # macOS Build (Universal)
  # ============================================
  build-mac:
    runs-on: macos-14  # Apple Silicon runner
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: GUI/package-lock.json

      - name: Download llama.cpp macOS binaries
        run: |
          # Metal (ARM64)
          curl -L -o llama-metal.tar.gz "https://github.com/ggml-org/llama.cpp/releases/download/${{ env.LLAMA_CPP_VERSION }}/llama-${{ env.LLAMA_CPP_VERSION }}-bin-macos-arm64.tar.gz"
          mkdir -p middleware/bin/darwin-metal
          tar -xzf llama-metal.tar.gz -C middleware/bin/darwin-metal --strip-components=1 || tar -xzf llama-metal.tar.gz -C middleware/bin/darwin-metal
          
          # å¤„ç†å¤šå±‚åµŒå¥—ç›®å½•
          if [ -d "middleware/bin/darwin-metal/build/bin" ]; then
            mv middleware/bin/darwin-metal/build/bin/* middleware/bin/darwin-metal/ 2>/dev/null || true
            rm -rf middleware/bin/darwin-metal/build
          elif [ -d "middleware/bin/darwin-metal/bin" ]; then
            mv middleware/bin/darwin-metal/bin/* middleware/bin/darwin-metal/ 2>/dev/null || true
            rm -rf middleware/bin/darwin-metal/bin
          fi
          
          # é€’å½’æŸ¥æ‰¾å¹¶ç§»åŠ¨ llama-server åˆ°é¡¶å±‚
          SERVER_PATH=$(find middleware/bin/darwin-metal -name "llama-server" -type f | head -1)
          if [ -n "$SERVER_PATH" ] && [ "$SERVER_PATH" != "middleware/bin/darwin-metal/llama-server" ]; then
            echo "Moving llama-server from $SERVER_PATH to top level"
            mv "$SERVER_PATH" middleware/bin/darwin-metal/
          fi
          chmod +x middleware/bin/darwin-metal/llama-server 2>/dev/null || true
          
          # x64 (Intel)
          curl -L -o llama-x64.tar.gz "https://github.com/ggml-org/llama.cpp/releases/download/${{ env.LLAMA_CPP_VERSION }}/llama-${{ env.LLAMA_CPP_VERSION }}-bin-macos-x64.tar.gz"
          mkdir -p middleware/bin/darwin-x64
          tar -xzf llama-x64.tar.gz -C middleware/bin/darwin-x64 --strip-components=1 || tar -xzf llama-x64.tar.gz -C middleware/bin/darwin-x64
          
          # å¤„ç†å¤šå±‚åµŒå¥—ç›®å½•
          if [ -d "middleware/bin/darwin-x64/build/bin" ]; then
            mv middleware/bin/darwin-x64/build/bin/* middleware/bin/darwin-x64/ 2>/dev/null || true
            rm -rf middleware/bin/darwin-x64/build
          elif [ -d "middleware/bin/darwin-x64/bin" ]; then
            mv middleware/bin/darwin-x64/bin/* middleware/bin/darwin-x64/ 2>/dev/null || true
            rm -rf middleware/bin/darwin-x64/bin
          fi
          
          # é€’å½’æŸ¥æ‰¾å¹¶ç§»åŠ¨ llama-server åˆ°é¡¶å±‚
          SERVER_PATH=$(find middleware/bin/darwin-x64 -name "llama-server" -type f | head -1)
          if [ -n "$SERVER_PATH" ] && [ "$SERVER_PATH" != "middleware/bin/darwin-x64/llama-server" ]; then
            echo "Moving llama-server from $SERVER_PATH to top level"
            mv "$SERVER_PATH" middleware/bin/darwin-x64/
          fi
          chmod +x middleware/bin/darwin-x64/llama-server 2>/dev/null || true
          
          # éªŒè¯äºŒè¿›åˆ¶å­˜åœ¨
          echo "=== Metal (ARM64) binaries ==="
          ls -la middleware/bin/darwin-metal/
          test -f middleware/bin/darwin-metal/llama-server && echo "âœ… Metal llama-server found" || echo "âŒ Metal llama-server NOT found"
          echo "=== x64 (Intel) binaries ==="
          ls -la middleware/bin/darwin-x64/
          test -f middleware/bin/darwin-x64/llama-server && echo "âœ… x64 llama-server found" || echo "âŒ x64 llama-server NOT found"

      - name: Setup uv
        uses: astral-sh/setup-uv@v5
        with:
          enable-cache: true

      - name: Install Python dependencies
        run: |
          python3 -m venv .venv
          source .venv/bin/activate
          pip install -r middleware/requirements.txt
          pip install -r middleware/server/requirements.txt

      - name: Build Python Engine (PyInstaller)
        run: |
          source .venv/bin/activate
          pip install pyinstaller
          cd middleware
          
          # åˆ›å»º PyInstaller spec ç”¨äºæ‰“åŒ…
          pyinstaller --onefile --name murasaki-engine \
            --hidden-import opencc \
            --hidden-import chardet \
            --hidden-import regex \
            --hidden-import pynvml \
            --hidden-import lxml \
            --hidden-import lxml.etree \
            --hidden-import ebooklib \
            --hidden-import ebooklib.epub \
            --collect-all opencc \
            --add-data "murasaki_translator:murasaki_translator" \
            --add-data "rule_processor.py:." \
            murasaki_translator/main.py
          
          # ç§»åŠ¨åˆ° bin ç›®å½•ä¾› Electron ä½¿ç”¨
          mkdir -p bin/python-bundle
          mv dist/murasaki-engine bin/python-bundle/
          chmod +x bin/python-bundle/murasaki-engine
          
          echo "Built Python bundle:"
          ls -la bin/python-bundle/

      - name: Install Node dependencies
        working-directory: GUI
        run: npm install

      - name: Build for ARM64
        working-directory: GUI
        run: |
          npm run build
          npx electron-builder --mac --arm64
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Build for x64
        working-directory: GUI
        run: npx electron-builder --mac --x64
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Upload Artifact
        uses: actions/upload-artifact@v4
        with:
          name: Murasaki-mac
          path: GUI/dist/*.dmg
          retention-days: 7

  # ============================================
  # Linux Build (GUI + CLI)
  # ============================================
  build-linux:
    runs-on: ubuntu-22.04
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: GUI/package-lock.json

      - name: Download llama.cpp Linux binaries
        run: |
          # Vulkan ç‰ˆæœ¬
          curl -L -o llama-vulkan.tar.gz "https://github.com/ggml-org/llama.cpp/releases/download/${{ env.LLAMA_CPP_VERSION }}/llama-${{ env.LLAMA_CPP_VERSION }}-bin-ubuntu-vulkan-x64.tar.gz"
          mkdir -p middleware/bin/linux-vulkan
          tar -xzf llama-vulkan.tar.gz -C middleware/bin/linux-vulkan --strip-components=1 || tar -xzf llama-vulkan.tar.gz -C middleware/bin/linux-vulkan
          
          # å¤„ç†å¤šå±‚åµŒå¥—ç›®å½• (build/bin/ æˆ–ç›´æ¥ bin/)
          if [ -d "middleware/bin/linux-vulkan/build/bin" ]; then
            mv middleware/bin/linux-vulkan/build/bin/* middleware/bin/linux-vulkan/ 2>/dev/null || true
            rm -rf middleware/bin/linux-vulkan/build
          elif [ -d "middleware/bin/linux-vulkan/bin" ]; then
            mv middleware/bin/linux-vulkan/bin/* middleware/bin/linux-vulkan/ 2>/dev/null || true
            rm -rf middleware/bin/linux-vulkan/bin
          fi
          
          # é€’å½’æŸ¥æ‰¾å¹¶ç§»åŠ¨ llama-server åˆ°é¡¶å±‚
          SERVER_PATH=$(find middleware/bin/linux-vulkan -name "llama-server" -type f | head -1)
          if [ -n "$SERVER_PATH" ] && [ "$SERVER_PATH" != "middleware/bin/linux-vulkan/llama-server" ]; then
            echo "Moving llama-server from $SERVER_PATH to top level"
            mv "$SERVER_PATH" middleware/bin/linux-vulkan/
          fi
          chmod +x middleware/bin/linux-vulkan/llama-server 2>/dev/null || true
          
          # CUDA ç‰ˆæœ¬ï¼ˆç”¨äº NVIDIA GPUï¼‰
          curl -L -o llama-cuda.tar.gz "https://github.com/ggml-org/llama.cpp/releases/download/${{ env.LLAMA_CPP_VERSION }}/llama-${{ env.LLAMA_CPP_VERSION }}-bin-ubuntu-cuda12.0-x64.tar.gz"
          mkdir -p middleware/bin/linux-cuda
          tar -xzf llama-cuda.tar.gz -C middleware/bin/linux-cuda --strip-components=1 || tar -xzf llama-cuda.tar.gz -C middleware/bin/linux-cuda
          
          # å¤„ç†å¤šå±‚åµŒå¥—ç›®å½•
          if [ -d "middleware/bin/linux-cuda/build/bin" ]; then
            mv middleware/bin/linux-cuda/build/bin/* middleware/bin/linux-cuda/ 2>/dev/null || true
            rm -rf middleware/bin/linux-cuda/build
          elif [ -d "middleware/bin/linux-cuda/bin" ]; then
            mv middleware/bin/linux-cuda/bin/* middleware/bin/linux-cuda/ 2>/dev/null || true
            rm -rf middleware/bin/linux-cuda/bin
          fi
          
          # é€’å½’æŸ¥æ‰¾å¹¶ç§»åŠ¨ llama-server åˆ°é¡¶å±‚
          SERVER_PATH=$(find middleware/bin/linux-cuda -name "llama-server" -type f | head -1)
          if [ -n "$SERVER_PATH" ] && [ "$SERVER_PATH" != "middleware/bin/linux-cuda/llama-server" ]; then
            echo "Moving llama-server from $SERVER_PATH to top level"
            mv "$SERVER_PATH" middleware/bin/linux-cuda/
          fi
          chmod +x middleware/bin/linux-cuda/llama-server 2>/dev/null || true
          
          # éªŒè¯äºŒè¿›åˆ¶å­˜åœ¨
          echo "=== Vulkan binaries ==="
          ls -la middleware/bin/linux-vulkan/
          test -f middleware/bin/linux-vulkan/llama-server && echo "âœ… Vulkan llama-server found" || echo "âŒ Vulkan llama-server NOT found"
          echo "=== CUDA binaries ==="
          ls -la middleware/bin/linux-cuda/
          test -f middleware/bin/linux-cuda/llama-server && echo "âœ… CUDA llama-server found" || echo "âŒ CUDA llama-server NOT found"

      - name: Setup uv
        uses: astral-sh/setup-uv@v5
        with:
          enable-cache: true

      - name: Install Python dependencies
        run: |
          python3 -m venv .venv
          source .venv/bin/activate
          pip install -r middleware/requirements.txt
          pip install -r middleware/server/requirements.txt

      - name: Build Python Engine (PyInstaller)
        run: |
          source .venv/bin/activate
          pip install pyinstaller
          cd middleware
          
          # åˆ›å»º PyInstaller spec ç”¨äºæ‰“åŒ…ï¼ˆä¸ macOS ç›¸åŒï¼‰
          pyinstaller --onefile --name murasaki-engine \
            --hidden-import opencc \
            --hidden-import chardet \
            --hidden-import regex \
            --hidden-import pynvml \
            --hidden-import lxml \
            --hidden-import lxml.etree \
            --hidden-import ebooklib \
            --hidden-import ebooklib.epub \
            --collect-all opencc \
            --add-data "murasaki_translator:murasaki_translator" \
            --add-data "rule_processor.py:." \
            murasaki_translator/main.py
          
          # ç§»åŠ¨åˆ° bin ç›®å½•ä¾› Electron ä½¿ç”¨
          mkdir -p bin/python-bundle
          mv dist/murasaki-engine bin/python-bundle/
          chmod +x bin/python-bundle/murasaki-engine
          
          echo "Built Python bundle:"
          ls -la bin/python-bundle/

      - name: Install Node dependencies
        working-directory: GUI
        run: npm install

      - name: Build GUI
        working-directory: GUI
        run: |
          npm run build
          npx electron-builder --linux --x64
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Package CLI Server
        run: |
          mkdir -p dist-cli/murasaki-server
          cp -r middleware/bin dist-cli/murasaki-server/
          cp -r middleware/murasaki_translator dist-cli/murasaki-server/
          cp -r middleware/openai_proxy dist-cli/murasaki-server/
          cp -r middleware/server dist-cli/murasaki-server/
          cp middleware/cli/murasaki_server.py dist-cli/murasaki-server/
          cp middleware/requirements.txt dist-cli/murasaki-server/
          # æ·»åŠ ç¼ºå¤±çš„ä¾èµ–æ–‡ä»¶
          cp middleware/rule_processor.py dist-cli/murasaki-server/
          # åˆå¹¶ requirementsï¼ˆç¡®ä¿æ¢è¡Œç¬¦ï¼‰
          echo "" >> dist-cli/murasaki-server/requirements.txt
          cat middleware/openai_proxy/requirements.txt >> dist-cli/murasaki-server/requirements.txt
          echo "" >> dist-cli/murasaki-server/requirements.txt
          cat middleware/server/requirements.txt >> dist-cli/murasaki-server/requirements.txt
          
          # åˆ›å»ºå¯åŠ¨è„šæœ¬
          cat > dist-cli/murasaki-server/start.sh << 'EOF'
          #!/bin/bash
          cd "$(dirname "$0")"
          
          # æ£€æŸ¥ Python ä¾èµ–
          if ! python3 -c "import fastapi" 2>/dev/null; then
            echo "Installing dependencies..."
            pip3 install -r requirements.txt
            pip3 install fastapi uvicorn httpx
          fi
          
          python3 murasaki_server.py "$@"
          EOF
          chmod +x dist-cli/murasaki-server/start.sh
          chmod +x dist-cli/murasaki-server/bin/linux-vulkan/llama-server
          chmod +x dist-cli/murasaki-server/bin/linux-cuda/llama-server
          
          # åˆ›å»º README
          cat > dist-cli/murasaki-server/README.md << 'EOF'
          # Murasaki Translator CLI Server
          
          OpenAI å…¼å®¹çš„ç¿»è¯‘ API æœåŠ¡å™¨ã€‚
          
          ## å¿«é€Ÿå¼€å§‹
          
          ```bash
          # 1. å®‰è£…ä¾èµ–
          pip3 install -r requirements.txt
          pip3 install fastapi uvicorn httpx
          
          # 2. è¿è¡ŒæœåŠ¡å™¨
          ./start.sh --model /path/to/model.gguf --port 8000
          
          # æˆ–è€…ç›´æ¥è¿è¡Œ
          python3 murasaki_server.py --model /path/to/model.gguf
          ```
          
          ## API ç«¯ç‚¹
          
          - `POST /v1/chat/completions` - Chat Completions (OpenAI å…¼å®¹)
          - `GET /v1/models` - æ¨¡å‹åˆ—è¡¨
          - `GET /health` - å¥åº·æ£€æŸ¥
          
          ## æµ‹è¯•
          
          ```bash
          curl http://localhost:8000/v1/chat/completions \
            -H "Content-Type: application/json" \
            -d '{"model":"local","messages":[{"role":"user","content":"ç¿»è¯‘ï¼šã“ã‚“ã«ã¡ã¯"}]}'
          ```
          EOF
          
          # æ·»åŠ  README å’Œ LICENSE
          cp .github/release-docs/README-linux-cli.md dist-cli/murasaki-server/README.md
          cp LICENSE dist-cli/murasaki-server/murasaki-translator.LICENSE.txt
          
          # æ‰“åŒ…
          cd dist-cli
          tar -czvf murasaki-server-linux-x64.tar.gz murasaki-server

      - name: Upload GUI Artifact
        uses: actions/upload-artifact@v4
        with:
          name: Murasaki-linux-x64
          path: GUI/dist/*.AppImage
          retention-days: 7

      - name: Upload CLI Artifact
        uses: actions/upload-artifact@v4
        with:
          name: Murasaki-server-linux-x64
          path: dist-cli/*.tar.gz

  # ============================================
  # Create GitHub Release
  # ============================================
  release:
    needs: [build-win-cuda, build-win-vulkan, build-mac, build-linux]
    runs-on: ubuntu-latest
    if: startsWith(github.ref, 'refs/tags/')
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts

      - name: Prepare release assets
        run: |
          mkdir -p release-assets
          # åªå¤åˆ¶éœ€è¦çš„æ–‡ä»¶æ ¼å¼
          find artifacts -type f \( -name "*.dmg" -o -name "*.zip" -o -name "*.AppImage" -o -name "*.tar.gz" \) -exec cp {} release-assets/ \;
          
          echo "Release assets:"
          ls -la release-assets/

      - name: Create GitHub Release
        uses: softprops/action-gh-release@v2
        with:
          files: release-assets/*
          draft: false
          prerelease: ${{ contains(github.ref_name, 'beta') || contains(github.ref_name, 'alpha') }}
          generate_release_notes: true
          body: |
            ## ğŸ“¦ ä¸‹è½½è¯´æ˜
            
            | å¹³å° | GPU | æ–‡ä»¶ |
            |------|-----|------|
            | Windows | NVIDIA (CUDA) | `*-win-cuda-x64.zip` |
            | Windows | AMD/Intel (Vulkan) | `*-win-vulkan-x64.zip` |
            | macOS | Apple Silicon | `*-arm64.dmg` |
            | macOS | Intel | `*.dmg` (æ—  arm64 åç¼€) |
            | Linux | GUI (Vulkan) | `*.AppImage` |
            | Linux | CLI æœåŠ¡å™¨ | `murasaki-server-*.tar.gz` |
            
            ## ğŸ–¥ï¸ Linux CLI æœåŠ¡å™¨
            
            ç”¨äºæ— ç•Œé¢æœåŠ¡å™¨éƒ¨ç½²ï¼Œæä¾› OpenAI å…¼å®¹ APIï¼š
            ```bash
            tar -xzf murasaki-server-linux-x64.tar.gz
            cd murasaki-server
            ./start.sh --model /path/to/model.gguf --port 8000
            ```
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
